# Лабораторная №3

[**Метод градиентного спуска**](#метод-градиентного-спуска)

[**Метод сопряжённых градиентов**](#метод-сопряжённых-градиентов)

[**Метод Ньютона-Рафсона**](#метод-ньютона-рафсона)

[**Метод штрафных функций**](#метод-штрафных-функций)

## Метод градиентного спуска

Градиентный спуск - это один из наиболее популярных методов оптимизации, который широко применяется в машинном обучении,
численном анализе и других областях. Этот метод особенно полезен, когда функция сложная и не имеет аналитического
решения.

Устройство алгоритма:

1. Метод принимает на вход функцию `function`, начальное значение `xStart`, точность `eps` и максимальное количество
   итераций
   `maxIterations`. Он возвращает вектор, который представляет точку, в которой был найден минимум функции.
2. Мы создаем два вектора: `tempX` и `tempNew`, инициализируя их значением `xStart`. Эти векторы будут использоваться
   для
   временного хранения текущей и новой оценок минимума функции.
3. Мы начинаем цикл, который продолжается до достижения максимального числа итераций.
4. Вычисляем градиент функции в текущей точке, используя функцию `calculateGradient`. Градиент представляет собой
   вектор, показывающий направление наискорейшего возрастания функции.
5. Мы перемещаемся в направлении, противоположном градиенту, на фиксированное расстояние `eps`. Это делается для того,
   чтобы найти новую оценку минимума.
6. Мы проверяем, достигли ли мы сходимости (т.е. новая оценка минимума достаточно близка к предыдущей). Если да,
   завершаем цикл.
7. Мы обновляем текущую оценку минимума, чтобы использовать ее в следующей итерации.
8. Возвращаем оценку минимума функции.

**Преимущества**:

- Простота реализации: Градиентный спуск относительно прост в реализации и понимании. Это один из наиболее
  распространенных методов оптимизации, который можно легко применить к различным задачам.
- Широкое применение: Градиентный спуск можно применять для оптимизации широкого спектра функций, включая выпуклые и
  невыпуклые функции.
- Эффективность: При правильном выборе параметров (например, скорости обучения) и грамотной настройке метод может
  сходиться к оптимальному решению достаточно быстро.

**Недостатки**:

- Сходимость к локальному минимуму: Градиентный спуск не гарантирует сходимость к глобальному минимуму функции. Он может
  застрять в локальных минимумах или седловых точках.
- Чувствительность к начальной точке: Результат градиентного спуска может сильно зависеть от начальной точки, с которой
  начинается оптимизация. Некоторые методы, такие как стохастический градиентный спуск, могут быть менее чувствительны к
  выбору начальной точки.
- Выбор скорости обучения: Выбор правильной скорости обучения (learning rate) является ключевым аспектом успешного
  применения градиентного спуска. Слишком большая скорость обучения может привести к расходимости, а слишком маленькая -
  к
  медленной сходимости.
- Вычислительная сложность: Градиентный спуск требует вычисления градиента функции, что может быть вычислительно
  затратной
  операцией для больших наборов данных или сложных функций.

## Метод сопряжённых градиентов

Метод сопряженных градиентов применяется во множестве областей, где требуется решение оптимизационных задач или систем
линейных уравнений. Например, в машинном обучении, обработке сигналов и изображений, компьютерной графике.

1. Создание копии начальной точки `xStart` для использования в качестве предыдущей точки.
2. Объявление переменных `xNext` для хранения следующей точки и `directionNew` для хранения нового направления.
3. Вычисление начального направления как противоположного градиенту функции в начальной точке. Градиент вычисляется с
   помощью функции `calculateGradient`, затем умножается на -1 для движения в направлении убывания функции.
4. Объявление переменной omega для хранения коэффициента, который будет использоваться для вычисления нового направления
   движения.
5. Инициализация счетчика итераций.
6. Начало цикла, который будет выполняться, пока не будет достигнуто максимальное количество итераций.
7. Вычисление следующей точки `xNext` с помощью метода `goldenRatioVector`, который использует метод золотого
   сечения для оптимизации. Этот метод используется для выбора следующей точки на основе текущей и направления движения.
8. Проверка условия остановки: если расстояние между текущей и предыдущей точками меньше удвоенной точности `eps`, то
   выход из цикла.
9. Вычисление нового градиента в точке `xNext`.
10. Вычисление коэффициента `omega` по формуле, который будет использоваться для обновления направления движения.
11. Обновление направления движения, умножив текущее направление на `omega` и вычитая из него новое направление.
12. Обновление предыдущей точки `xPrev` до текущей точки `xNext` перед переходом к следующей итерации.
13. Увеличение счетчика итераций.
14. Возвращение найденной минимальной точки `xPrev`.

**Преимущества**:

- Быстрота сходимости: Метод сопряженных градиентов обычно сходится быстрее, чем обычные методы градиентного спуска,
  особенно для квадратичных функций.
- Эффективность для квадратичных функций: Для квадратичных функций метод сопряженных градиентов дает точное решение за
  конечное число шагов (в точности n, где n - размерность пространства).
- Экономия памяти: Метод сопряженных градиентов требует хранения только нескольких векторов, что делает его экономичным
  по использованию памяти.
  Подходит для больших систем: Метод сопряженных градиентов подходит для решения систем линейных уравнений с большим
  числом неизвестных.

**Недостатки**:

- Чувствительность к выбору начальной точки: В некоторых случаях метод сопряженных градиентов может быть чувствителен к
  выбору начальной точки, особенно для нелинейных функций.
- Не всегда гарантирована сходимость: Для некоторых функций и систем уравнений метод сопряженных градиентов может не
  сойтись к оптимальному решению или сделать это очень медленно.
- Ограничения на класс задач: Хотя метод сопряженных градиентов может быть эффективным для квадратичных функций и
  некоторых других задач, он не всегда подходит для общих нелинейных задач оптимизации.
- Не всегда устойчив к шуму: Метод сопряженных градиентов может быть неустойчив к шуму в данных или плохо обусловленным
  матрицам, что может замедлить сходимость или привести к ошибкам.

## Метод Ньютона-Рафсона

Этот метод использует итеративный процесс для поиска локального минимума функции с использованием градиента и гессиана
функции.

1. Создается копия вектора `xStart`, чтобы не изменять оригинальный вектор при выполнении метода.
2. Объявляются переменные `hessian`, `gradient` и `iter`. `hessian` будет использоваться для хранения гессиана
   функции, `gradient` - градиента функции, а `iter` - счетчика итераций.
3. Создаём цикл, который будет выполняться, пока количество выполненных итераций не достигнет максимального значения.
4. Вычисляется градиент функции в точке `x` с использованием функции `calculateGradient`.
5. Вычисляется гессиан функции в точке x с использованием функции `calculateHessian`.
6. Вычисляется направление движения в методе Ньютона-Рафсона. Для этого сначала решается система линейных уравнений,
   используя LU-разложение гессиана. Решение этой системы дает направление движения.
7. Проверяется условие остановки. Если расстояние между текущей точкой `x` и предыдущей точкой меньше, чем `2 * eps`, то
   выход из цикла.
8. Обновляется текущая точка `x` путем добавления вычисленного направления.
9. Увеличивается счетчик итераций.
10. Возвращается найденная точка.

**Преимущества**:

- Быстрая сходимость: Метод Ньютона-Рафсона сходится очень быстро к локальному минимуму функции в сравнении с простыми
  методами градиентного спуска. Это особенно заметно в окрестности точки минимума, где функция имеет квадратичную форму.
- Эффективное использование информации о кривизне: Метод Ньютона-Рафсона использует информацию о кривизне функции,
  представленной гессианом, что позволяет ему эффективно находить минимумы в сложных и выпуклых функциях.
- Глобальная сходимость в квадратичных функциях: В случае квадратичных функций метод Ньютона-Рафсона сходится к
  глобальному минимуму за одну итерацию, что делает его эффективным.

**Недостатки**:

- Сложность вычисления гессиана: Вычисление гессиана функции может быть вычислительно затратной операцией, особенно если
  функция имеет большое количество переменных или сложную структуру.
- Неустойчивость вблизи точек экстремума: В случае, если начальное приближение находится слишком близко к точке
  минимума, метод Ньютона-Рафсона может быть неустойчивым и даже расходиться.
- Требует дифференцируемости функции: Метод Ньютона-Рафсона требует, чтобы функция была дважды непрерывно
  дифференцируема, что может быть ограничением в некоторых приложениях, особенно если функция содержит разрывы или
  особые точки.
- Чувствительность к выбору начального приближения: Эффективность метода Ньютона-Рафсона сильно зависит от выбора
  начального приближения, и неправильный выбор может привести к медленной сходимости или даже к расходимости.

## Метод штрафных функций

Этот метод использует штрафные функции для учета ограничений при поиске минимума функции, применяя штрафы к градиентам
функций и ограничений. Таким образом, цель состоит в том, чтобы минимизировать функцию с учетом всех заданных
ограничений.

1. Создание копии начального приближения `xStart`, чтобы не изменять исходное значение.
2. Инициализация переменной `iter`, которая будет отслеживать количество выполненных итераций.
3. Инициализация переменной `penalty`, которая представляет величину штрафа для нарушения ограничений.
4. Начало цикла, выполняющегося до достижения максимального числа итераций.
5. Вычисление градиента функции `function` в точке `x` с помощью функции `calculateGradient`.
6. Вычисление гессиана функции `function` в точке `x` с помощью функции `calculateHessian`.
7. Вычисление градиента функции ограничений `constraintFunction` в точке `x`.
8. Вычисление комбинированного градиента `penaltyGradient`, который представляет собой сумму градиента
   функции `function` и градиента функции ограничений, умноженного на штраф.
9. Вычисление направления движения путем решения системы линейных уравнений, полученных из гессиана и комбинированного
   градиента.
10. Проверка условия остановки: если расстояние между текущей точкой `x` и предыдущей точкой меньше двойной
    точности `eps`, то прерываем цикл.
11. Обновление текущей точки `x`, прибавляя вычисленное направление движения.
12. Увеличение счетчика итераций и завершение цикла.
13. Возвращение найденной минимальной точки в качестве результата работы метода.

**Преимущества**:

- Универсальность: Метод штрафных функций применим для решения широкого спектра задач оптимизации с ограничениями,
  включая линейные и нелинейные ограничения.
- Простота реализации: Он относительно прост в реализации и не требует сложных математических методов или структур
  данных.
- Глобальная оптимизация: Метод штрафных функций позволяет находить глобальные минимумы функции с учетом всех
  ограничений.

**Недостатки**:

- Чувствительность к параметрам: Эффективность метода может сильно зависеть от выбора параметров, таких как величина
  штрафа и начальное значение штрафа.
- Возможность зацикливания: В некоторых случаях метод может зацикливаться в окрестности точек, нарушающих ограничения,
  что приводит к длительному выполнению итераций.
- Потеря точности: При использовании больших значений штрафа может происходить потеря точности, так как штраф может
  приводить к сильному увеличению градиента и изменению направления поиска минимума.
- Вычислительная сложность: Для решения задачи с использованием метода штрафных функций требуется выполнение нескольких
  итераций вычислений, что может привести к высокой вычислительной нагрузке.